{
  "agents": {
    "Architect": {
      "model": {
        "name": "Mistral 7B",
        "reasoning": "\u00dcbertrifft Llama 2 13B auf vielen Benchmarks, effizient f\u00fcr lange Sequenzen und gute generelle Sprache/Architektur-Analyse. CPU-tauglich f\u00fcr moderate Latenz; kann auch auf GPU laufen wenn verf\u00fcgbar. ",
        "sources": [
          ":contentReference[oaicite:0]{index=0}",
          ":contentReference[oaicite:1]{index=1}"
        ],
        "type": "open-source generalist mit starker Performance auf Design-/Analyse-Aufgaben"
      },
      "models": [
        "qwen3-zero-coder-reasoning-0.8b-neo-ex"
      ],
      "template": "Architect",
      "max_summary_length": 300,
      "step_delay": 10,
      "auto_restart": false,
      "allow_commands": true,
      "controller_active": true,
      "prompt": "",
      "tasks": [],
      "purpose": "System- und Hoheniveau-Design, Schnittstellen, Modulaufteilung, Tech-Stack-Auswahl",
      "preferred_hardware": "CPU"
    },
    "Backend Developer": {
      "model": {
        "name": "StarCoder2-7B oder 15B (je nach Ressourcen)",
        "type": "code-specialisiertes LLM",
        "reasoning": "F\u00fcr robuste Code-Generierung und -Erg\u00e4nzung in vielen Sprachen; Fill-in-the-Middle und gro\u00dfer Kontext. GPU via Ollama f\u00fcr Performance. Kleinere Variante (7B) passt besser zur RTX3080 bei quantisierten Ausf\u00fchrungen. ",
        "sources": [
          ":contentReference[oaicite:2]{index=2}",
          ":contentReference[oaicite:3]{index=3}",
          ":contentReference[oaicite:4]{index=4}"
        ]
      },
      "models": [],
      "template": "Backend Developer",
      "max_summary_length": 300,
      "step_delay": 10,
      "auto_restart": false,
      "allow_commands": true,
      "controller_active": true,
      "prompt": "",
      "tasks": [],
      "purpose": "Implementierung der Serverlogik, APIs, Gesch\u00e4ftsregeln, Datenpersistenz",
      "preferred_hardware": "GPU"
    },
    "Frontend Developer": {
      "model": {
        "name": "CodeLlama (z.B. CodeLlama 13B oder 7B)",
        "type": "code-fokussiertes LLM spezialisiert auf Frontend/JS/HTML/CSS",
        "reasoning": "CodeLlama ist auf Code und Entwickler-Workflows optimiert, erzeugt sauberen Frontend-Code und Erkl\u00e4rungen. GPU-Nutzung f\u00fcr interaktive Generierung. ",
        "sources": [
          ":contentReference[oaicite:5]{index=5}",
          ":contentReference[oaicite:6]{index=6}",
          ":contentReference[oaicite:7]{index=7}"
        ]
      },
      "models": [],
      "template": "Frontend Developer",
      "max_summary_length": 300,
      "step_delay": 10,
      "auto_restart": false,
      "allow_commands": true,
      "controller_active": true,
      "prompt": "",
      "tasks": [],
      "purpose": "UI/UX-Implementierung, Schnittstellen-Integration, state management",
      "preferred_hardware": "GPU"
    },
    "Fullstack Reviewer": {
      "model": {
        "name": "Llama 2 7B (lokal via llama.cpp) oder StarCoder2-3B f\u00fcr leichteres Review",
        "type": "generalist / code-aware",
        "reasoning": "Kleinere LLMs \u00fcber llama.cpp laufen gut auf CPU f\u00fcr Review-Aufgaben; Llama 2 ist breit einsetzbar und mit Code-Kontext kombinierbar. ",
        "sources": [
          ":contentReference[oaicite:8]{index=8}"
        ]
      },
      "models": [],
      "template": "Fullstack Reviewer",
      "max_summary_length": 300,
      "step_delay": 10,
      "auto_restart": false,
      "allow_commands": true,
      "controller_active": true,
      "prompt": "",
      "tasks": [],
      "purpose": "Code-Review, Konsistenz zwischen Front- und Backend, Security-Checks, Best Practices",
      "preferred_hardware": "CPU"
    },
    "QA/Test Engineer": {
      "model": {
        "name": "Llama 2 7B oder StarCoder2-3B",
        "type": "code / reasoning",
        "reasoning": "Leichtgewichtige Modelle reichen f\u00fcr Testfall-Generierung und Testpl\u00e4ne; laufen zuverl\u00e4ssig auf CPU. ",
        "sources": [
          ":contentReference[oaicite:9]{index=9}"
        ]
      },
      "models": [],
      "template": "QA/Test Engineer",
      "max_summary_length": 300,
      "step_delay": 10,
      "auto_restart": false,
      "allow_commands": true,
      "controller_active": true,
      "prompt": "",
      "tasks": [],
      "purpose": "Testfall-Generierung, automatisierte Tests, Teststrategie, Edge-Cases",
      "preferred_hardware": "CPU"
    },
    "DevOps Engineer": {
      "model": {
        "name": "Mistral 7B oder Llama 2 7B",
        "type": "generalist mit DevOps-know-how",
        "reasoning": "Geeignet f\u00fcr Skript-Generierung (Terraform, GitHub Actions, Docker, K8s) und Ops-Strategien auf CPU. ",
        "sources": [
          ":contentReference[oaicite:10]{index=10}",
          ":contentReference[oaicite:11]{index=11}"
        ]
      },
      "models": [],
      "template": "DevOps Engineer",
      "max_summary_length": 300,
      "step_delay": 10,
      "auto_restart": false,
      "allow_commands": true,
      "controller_active": true,
      "prompt": "",
      "tasks": [],
      "purpose": "Deployment, CI/CD, Infrastruktur als Code, Monitoring, Rollback-Strategien",
      "preferred_hardware": "CPU"
    },
    "Scrum Master / Product Owner": {
      "model": {
        "name": "Llama 2 7B",
        "type": "generalist",
        "reasoning": "F\u00fcr Organisation, Formulierung von User Stories, Akzeptanzkriterien und Kommunikation ausreichend auf CPU. ",
        "sources": [
          ":contentReference[oaicite:12]{index=12}"
        ]
      },
      "models": [],
      "template": "Scrum Master / Product Owner",
      "max_summary_length": 300,
      "step_delay": 10,
      "auto_restart": false,
      "allow_commands": true,
      "controller_active": true,
      "prompt": "",
      "tasks": [],
      "purpose": "Backlog-Pflege, Priorisierung, Stakeholder-Kommunikation, Sprint-Planning",
      "preferred_hardware": "CPU"
    }
  },
  "active_agent": "Architect",
  "prompt_templates": {
    "Architect": "{task}"
  },
  "api_endpoints": [
    {
      "type": "lmstudio",
      "url": "http://host.docker.internal:1234/v1/chat/completions",
      "models": []
    }
  ],
  "tasks": [],
  "pipeline_order": [
    "Architect",
    "Backend Developer",
    "Frontend Developer",
    "Fullstack Reviewer",
    "QA/Test Engineer",
    "DevOps Engineer",
    "Scrum Master / Product Owner"
  ],
  "models": [
    "m1",
    "m2"
  ]
}
